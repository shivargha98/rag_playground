{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5ed6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "llm_model = ChatGoogleGenerativeAI(model = 'gemini-2.0-flash',max_retries=2)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2') ##embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280dac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 45\n",
      "Chunks : \n",
      "Chunk number: 1\n",
      "Chunk content : LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes\n",
      "Chunk number: 2\n",
      "Chunk content : to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "Chunk number: 3\n",
      "Chunk content : LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use\n",
      "Chunk number: 4\n",
      "Chunk content : models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "Chunk number: 5\n",
      "Chunk content : Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using\n",
      "Chunk number: 6\n",
      "Chunk content : LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "Chunk number: 7\n",
      "Chunk content : BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n",
      "Chunk number: 8\n",
      "Chunk content : Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\n",
      "Chunk number: 9\n",
      "Chunk content : LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\n",
      "Chunk number: 10\n",
      "Chunk content : FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document\n",
      "Chunk number: 11\n",
      "Chunk content : which makes it scalable for large document stores.\n",
      "Chunk number: 12\n",
      "Chunk content : Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "Chunk number: 13\n",
      "Chunk content : LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Chunk number: 14\n",
      "Chunk content : Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "Chunk number: 15\n",
      "Chunk content : LangChain’s architecture is modular, allowing developers to mix and match components based on their specific application needs.\n",
      "Chunk number: 16\n",
      "Chunk content : Prompt templates help standardize and reuse prompts efficiently, reducing redundancy and human error.\n",
      "Chunk number: 17\n",
      "Chunk content : Chains combine multiple components into a single workflow, passing outputs from one step as inputs to another. This enables complex reasoning pipelines, such as document retrieval followed by answer\n",
      "Chunk number: 18\n",
      "Chunk content : such as document retrieval followed by answer synthesis.\n",
      "Chunk number: 19\n",
      "Chunk content : For example, a user query like “Summarize the latest financial report of Tesla” can trigger a retrieval chain that fetches relevant documents, processes them through a summarization model, and\n",
      "Chunk number: 20\n",
      "Chunk content : processes them through a summarization model, and outputs a concise response.\n",
      "Chunk number: 21\n",
      "Chunk content : The framework also supports Custom Tools, which allow developers to define APIs or functions that an agent can call when reasoning.\n",
      "Chunk number: 22\n",
      "Chunk content : LangChain’s ToolExecutor module handles execution logic and result parsing, ensuring smooth integration between LLM reasoning and real-world data sources.\n",
      "Chunk number: 23\n",
      "Chunk content : It also provides Callback Handlers, which enable logging, monitoring, and visualization of intermediate steps. This feature is particularly useful for debugging or optimizing chains.\n",
      "Chunk number: 24\n",
      "Chunk content : Vector databases such as Chroma, Weaviate, Pinecone, and FAISS are integral to LangChain’s RAG pipelines.\n",
      "Chunk number: 25\n",
      "Chunk content : They store document embeddings generated from transformer-based models such as OpenAI’s text-embedding-ada-002 or Sentence Transformers.\n",
      "Chunk number: 26\n",
      "Chunk content : When a user query arrives, it is embedded in the same space, and the nearest documents are retrieved for contextual grounding.\n",
      "Chunk number: 27\n",
      "Chunk content : This allows the LLM to provide factually accurate and up-to-date answers — even when the base model’s training data is outdated.\n",
      "Chunk number: 28\n",
      "Chunk content : LangChain also integrates with LangSmith, a companion platform that enables debugging, tracing, and evaluating LLM applications in production.\n",
      "Chunk number: 29\n",
      "Chunk content : It tracks latency, prompt costs, and chain efficiency — helping developers identify bottlenecks in their workflow.\n",
      "Chunk number: 30\n",
      "Chunk content : With LangSmith, one can visualize each prompt-response pair, see how the model’s reasoning evolves, and improve performance over time.\n",
      "Chunk number: 31\n",
      "Chunk content : Another advanced feature is RetrievalQA, a pre-built chain that combines retrieval and question answering seamlessly.\n",
      "Chunk number: 32\n",
      "Chunk content : It is widely used in chatbots, enterprise knowledge systems, and document assistants.\n",
      "Chunk number: 33\n",
      "Chunk content : Developers can also customize retrieval strategies — for instance, using reranking models or hybrid retrievers to enhance answer relevance.\n",
      "Chunk number: 34\n",
      "Chunk content : LangChain’s memory system is flexible — ranging from short-term conversation buffers to long-term vector stores.\n",
      "Chunk number: 35\n",
      "Chunk content : For chatbots, the buffer memory retains the last few interactions, ensuring continuity.\n",
      "Chunk number: 36\n",
      "Chunk content : For task-oriented agents, entity memory stores structured data about ongoing processes or user-specific preferences.\n",
      "Chunk number: 37\n",
      "Chunk content : This design makes LangChain suitable for building personal assistants, copilots, and customer service bots.\n",
      "Chunk number: 38\n",
      "Chunk content : LangChain agents can operate in various modes such as zero-shot-react-description, plan-and-execute, or conversational-react-description.\n",
      "Chunk number: 39\n",
      "Chunk content : In the first mode, the agent decides its next step purely based on prompt reasoning.\n",
      "In the plan-and-execute mode, it first generates a high-level plan and then executes each subtask.\n",
      "Chunk number: 40\n",
      "Chunk content : This modular decision-making framework gives agents the flexibility to adapt to complex workflows like research synthesis or API orchestration.\n",
      "Chunk number: 41\n",
      "Chunk content : Beyond text, LangChain supports multimodal integrations.\n",
      "Developers can connect image understanding models, speech-to-text converters, or structured data analysis tools.\n",
      "Chunk number: 42\n",
      "Chunk content : This enables hybrid applications — for example, visual question answering, document parsing, or report generation from spreadsheets.\n",
      "Chunk number: 43\n",
      "Chunk content : Finally, LangChain’s open-source ecosystem continues to grow rapidly.\n",
      "Chunk number: 44\n",
      "Chunk content : Its compatibility with modern frameworks like LangGraph, LlamaIndex, and Chainlit enables developers to build interactive, agentic, and retrieval-based systems effortlessly.\n",
      "Chunk number: 45\n",
      "Chunk content : With strong community support, detailed documentation, and continuous innovation, LangChain has become a cornerstone of the modern AI application development stack.\n"
     ]
    }
   ],
   "source": [
    "##doc loader##\n",
    "loader = TextLoader(\n",
    "    file_path='data/langchain_sample.txt',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "doc = loader.load()\n",
    "\n",
    "##text splitter##\n",
    "chunker = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 50,\n",
    "    separators = [\"\\n\",\" \"]\n",
    ")\n",
    "doc_chunks = chunker.split_documents(doc)\n",
    "print(f\"Number of chunks created: {len(doc_chunks)}\")\n",
    "print(f\"Chunks : \")\n",
    "for index,chunk in enumerate(doc_chunks):\n",
    "    print(f\"Chunk number: {index+1}\")\n",
    "    print(f\"Chunk content : {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01519b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='bbda434d-2133-41aa-95a0-66d0e537da62', metadata={'source': 'data/langchain_sample.txt'}, page_content='Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='875f1dd8-a808-4411-ac26-c8d8db48abfd', metadata={'source': 'data/langchain_sample.txt'}, page_content='LangChain agents can operate in various modes such as zero-shot-react-description, plan-and-execute, or conversational-react-description.'),\n",
       " Document(id='50afd39c-8619-4f8c-8707-acc1d63b005d', metadata={'source': 'data/langchain_sample.txt'}, page_content='With strong community support, detailed documentation, and continuous innovation, LangChain has become a cornerstone of the modern AI application development stack.'),\n",
       " Document(id='71d10f58-997a-4883-a717-e067e644efeb', metadata={'source': 'data/langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes'),\n",
       " Document(id='1fad0405-2078-4ff6-9bbd-9157ddd70b25', metadata={'source': 'data/langchain_sample.txt'}, page_content='This design makes LangChain suitable for building personal assistants, copilots, and customer service bots.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vectore store as retreiver ##\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding = embedding_model\n",
    ")\n",
    "retreiver = vector_store.as_retriever(search_kwargs={'k':5})\n",
    "\n",
    "##retreiver test##\n",
    "query = 'Agentic AI capabilities in Langchain'\n",
    "retreiver.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction ='''\n",
    "You are an intelligent re-ranking model. Your task is to carefully evaluate and reorder a \\\n",
    "list of retrieved documents based on their relevance to the user's query.\n",
    "The documents will be numbered.\n",
    "The response should be only the document numbers in re-ranked state.\n",
    "\n",
    "Context_docuements : {context}\n",
    "User_Query : {user_query}\n",
    "\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    instruction\n",
    ")\n",
    "\n",
    "class response_structure(BaseModel):\n",
    "    re_ranked_response:str = Field(...,description=\"Respond with re-ranked document numbers, no text required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee16c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def format_docs(document_list)->List[str]:\n",
    "    '''Fomrat documents to create a context for the LLM, numbers will be added in the beginning of the chunks'''\n",
    "    doc_list = []\n",
    "    for index,doc in enumerate(document_list):\n",
    "        indexed_doc = f\"{index+1}. {doc}\"\n",
    "        doc_list.append(indexed_doc)\n",
    "\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd4e54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context':retreiver | format_docs, 'user_query':RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_model.with_structured_output(response_structure)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abc45dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000274811D4640>, search_kwargs={'k': 5})\n",
       "           | RunnableLambda(format_docs)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'user_query'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'user_query'], input_types={}, partial_variables={}, template=\"\\nYou are an intelligent re-ranking model. Your task is to carefully evaluate and reorder a list of retrieved documents based on their relevance to the user's query.\\nThe documents will be numbered.\\nThe response should be only the numbers re-ranked.\\n\\nContext : {context}\\nUser_Query : {user_query}\\n\\n\"), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000274E029A900>, default_metadata=(), model_kwargs={}), kwargs={'tools': [{'type': 'function', 'function': {'name': 'response_structure', 'description': '', 'parameters': {'properties': {'re_ranked_response': {'description': 'Respond with re-ranked document numbers, no text required', 'type': 'string'}}, 'required': ['re_ranked_response'], 'type': 'object'}}}], 'ls_structured_output_format': {'kwargs': {'method': 'function_calling'}, 'schema': {'type': 'function', 'function': {'name': 'response_structure', 'description': '', 'parameters': {'properties': {'re_ranked_response': {'description': 'Respond with re-ranked document numbers, no text required', 'type': 'string'}}, 'required': ['re_ranked_response'], 'type': 'object'}}}}, 'tool_choice': 'response_structure'}, config={}, config_factories=[])\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.response_structure'>])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1352c851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response_structure(re_ranked_response='1, 2, 5, 4, 3')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke('Agentic AI capabilities in Langchain')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa601759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
