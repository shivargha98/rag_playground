{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a32407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG_course\\rag_playground\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## imports ##\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader,DirectoryLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d730ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb2fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded : 3\n",
      "Metadata of the document : {'source': 'data\\\\chatgpt_conversational_ai.txt'}\n",
      "Document content:\n",
      "ChatGPT and Conversational AI\n",
      "\n",
      "ChatGPT is one of the most widely known conversational AI models deve\n",
      "-------------------------------\n",
      "Metadata of the document : {'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Document content:\n",
      "Google Gemini and the Rise of Multimodal AI\n",
      "\n",
      "Google Gemini represents a new generation of large-scal\n",
      "-------------------------------\n",
      "Metadata of the document : {'source': 'data\\\\machine_learning_fundamentals.txt'}\n",
      "Document content:\n",
      "Machine Learning Fundamentals\n",
      "\n",
      "Machine Learning (ML) is a branch of artificial intelligence focused \n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "## document loaders ##\n",
    "loader = DirectoryLoader(\n",
    "    path='data',\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'}\n",
    ")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents loaded : {len(documents)}\")\n",
    "for index,doc in enumerate(documents):\n",
    "    print(f\"Metadata of the document : {doc.metadata}\")\n",
    "    print(f\"Document content:\\n{doc.page_content[0:100]}\")\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07bd0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created : 12\n",
      "Content in chunk : architecture but extends it with multimodal embeddings. These embeddings allow the model to represen\n",
      "Metadata in a chunk:{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n"
     ]
    }
   ],
   "source": [
    "## document splitters ##\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size = 500, ##max size of one chunk\n",
    "    chunk_overlap = 50, ##chunk overlap character count\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of chunks created : {len(chunks)}\")\n",
    "print(f\"Content in chunk : {chunks[6].page_content[0:100]}\")\n",
    "print(f\"Metadata in a chunk:{chunks[6].metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e904d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 24 vectors\n"
     ]
    }
   ],
   "source": [
    "### chromadb vector store ###\n",
    "persist_dir = \"./chroma_db\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2') ##embedding model\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name='rag_collection'\n",
    ")\n",
    "print(f\"Vector store created with {vector_store._collection.count()} vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55fb94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number-1\n",
      "chunk metadata:\n",
      "{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Data in chunk:\n",
      "to systems capable of general intelligence. Gemini’s release positions Google as a key competitor to OpenAI’s GPT-4 and Anthropic’s Claude models. As multimodal AI continues to mature, we can expect breakthroughs in robotics, education, and accessibility—where machines can truly “see,” “hear,” and “understand” the world like humans do.\n",
      "----------------------------\n",
      "chunk number-2\n",
      "chunk metadata:\n",
      "{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Data in chunk:\n",
      "to systems capable of general intelligence. Gemini’s release positions Google as a key competitor to OpenAI’s GPT-4 and Anthropic’s Claude models. As multimodal AI continues to mature, we can expect breakthroughs in robotics, education, and accessibility—where machines can truly “see,” “hear,” and “understand” the world like humans do.\n",
      "----------------------------\n",
      "chunk number-3\n",
      "chunk metadata:\n",
      "{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Data in chunk:\n",
      "traditional LLMs that are text-only, Gemini can interpret visual inputs, perform cross-modal reasoning, and produce outputs that combine modalities. This makes it ideal for use cases like visual question answering, document analysis, or creative generation. It also integrates with Google Search, Workspace, and YouTube, enabling intelligent summarization, content generation, and recommendation systems.\n",
      "\n",
      "Technically, Gemini builds upon the Transformer architecture but extends it with multimodal\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "## testing the similarity search##\n",
    "\n",
    "query = 'What capabilities does Google gemini have?'\n",
    "sim_chunks = vector_store.similarity_search(query,k=3)\n",
    "for index,chnk in enumerate(sim_chunks):\n",
    "    print(f\"chunk number-{index+1}\")\n",
    "    print(f\"chunk metadata:\\n{chnk.metadata}\")\n",
    "    print(f\"Data in chunk:\\n{chnk.page_content}\")\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0b685",
   "metadata": {},
   "source": [
    "### RAG PIPELINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a82b3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm_model = ChatGoogleGenerativeAI(model = 'gemini-2.0-flash',max_retries=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create vector store retreiver ##\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs = {'k':3} ##top3 similar chunks will be retrieved\n",
    ")\n",
    "## retriever acts as the frontend of the vector store or a link for the vector store ##\n",
    "\n",
    "## system prompt ##\n",
    "system_prompt = '''You are an intelligent assisstant, for question-answering tasks,\n",
    "Use the retrieved context given below to answer the question in a more streamlined way.\n",
    "Answer in only 3 sentences\n",
    "\n",
    "Context : {context}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b38671b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027F38C14830>, search_kwargs={'k': 3}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are an intelligent assisstant, for question-answering tasks,\\nUse the retrieved context given below to answer the question in a more streamlined way.\\nAnswer in only 3 sentences\\n\\nContext : {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000027F47A45A70>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm_model,prompt)\n",
    "rag_pipeline = create_retrieval_chain(retriever,document_chain) ##linking the retreiver to the augmentation part##\n",
    "rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8676cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Question: What are the capabilities of Chatgpt?\n",
      "Number of chunks retrieved: 3\n",
      "Response:ChatGPT is a conversational AI system designed to mimic human dialogue. It can answer questions, draft emails, and write code. Its strength lies in its ability to generalize from patterns in data, enabling it to handle a wide variety of topics without task-specific training.\n"
     ]
    }
   ],
   "source": [
    "query = 'What are the capabilities of Chatgpt?'\n",
    "response = rag_pipeline.invoke({'input':query})\n",
    "print(f\"Input Question: {query}\")\n",
    "print(f\"Number of chunks retrieved: {len(response['context'])}\")\n",
    "print(f\"Response:{response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d01f25",
   "metadata": {},
   "source": [
    "### USING LCEL\n",
    "\n",
    "Query -> Embeddings -> VectorStore -> Context ->Augmentation ->LLM Generation -> Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6df93bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    '''Creating a context from chunks'''\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are an intelligent assisstant, for question-answering tasks,\n",
    "Use the retrieved context given below to answer the question in a more streamlined way.\n",
    "Answer in only 3 sentences\n",
    "\n",
    "Context : {context}\n",
    "Question : {question}\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d549fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Machine Learning (ML) is a field of artificial intelligence that focuses on creating systems capable of learning from data without explicit programming. It enables computers to identify patterns and make predictions based on experience. ML algorithms are categorized into supervised, unsupervised, and reinforcement learning.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "rag_chain = (\n",
    "    {\"context\":retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "## When using RunnablePassThrough - no need of using key value pair ##\n",
    "rag_chain.invoke(\"What are ML fundamentals?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af59cb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\machine_learning_fundamentals.txt'}, page_content='Machine Learning Fundamentals\\n\\nMachine Learning (ML) is a branch of artificial intelligence focused on building systems that learn from data rather than being explicitly programmed. The core idea is to enable computers to identify patterns and make predictions based on experience. ML algorithms can be broadly divided into supervised, unsupervised, and reinforcement learning.\\n\\nIn supervised learning, models are trained on labeled data, meaning that each training example has a known output.'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_fundamentals.txt'}, page_content='Machine Learning Fundamentals\\n\\nMachine Learning (ML) is a branch of artificial intelligence focused on building systems that learn from data rather than being explicitly programmed. The core idea is to enable computers to identify patterns and make predictions based on experience. ML algorithms can be broadly divided into supervised, unsupervised, and reinforcement learning.\\n\\nIn supervised learning, models are trained on labeled data, meaning that each training example has a known output.'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_fundamentals.txt'}, page_content='which uses multi-layer neural networks to process complex data such as images or speech. The integration of ML into industries like healthcare, finance, and marketing has transformed decision-making and automation. As data grows exponentially, the emphasis is now on ethical AI, model interpretability, and reducing bias to ensure that ML systems remain trustworthy and beneficial to society.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What are ML fundamentals?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef056f",
   "metadata": {},
   "source": [
    "### Adding new files to Vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38180dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded : Convolutional Neural Networks (CNNs)\n",
      "\n",
      "Convolutional Neural Networks, or CNNs, are a class of deep le\n",
      "Metadata: {'source': 'data//convolutional_neural_networks.txt'}\n"
     ]
    }
   ],
   "source": [
    "## text loaders##\n",
    "text_loader = TextLoader('data//convolutional_neural_networks.txt',encoding='utf-8')\n",
    "data = text_loader.load()\n",
    "print(f\"Data Loaded : {data[0].page_content[0:100]}\")\n",
    "print(f\"Metadata: {data[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87ed1aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New chunks created : 5\n",
      "Chunk example:\n",
      "Convolutional Neural Networks (CNNs)\n",
      "\n",
      "Convolutional Neural Networks, or CNNs, are a class of deep learning models specifically designed for processing structured grid-like data such as images and videos. They are the backbone of modern computer vision systems, powering tasks like image classification, object detection, facial recognition, and medical image analysis.\n",
      "\n",
      "The core idea behind CNNs is the use of convolutional layers that apply filters (also called kernels) over the input data to\n"
     ]
    }
   ],
   "source": [
    "### chunking ##\n",
    "new_doc_chunks = text_splitter.split_documents(data)\n",
    "print(f\"New chunks created : {len(new_doc_chunks)}\")\n",
    "print(f\"Chunk example:\\n{new_doc_chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86a6ddcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d7ad86fa-98a5-401e-8775-62fefc38cf35',\n",
       " '311b7ff4-7ad9-4445-a80b-21642f38b8be',\n",
       " '15abc462-1195-460b-ad6f-f3e11b8d1a93',\n",
       " '75fe8943-f5ed-41a8-a10c-66a4de7bb8e3',\n",
       " '920e9a84-3bac-41fe-875b-81c7af9fe3dd']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(new_doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8b4eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: CNNs excel at spatial understanding but struggle with sequential or contextual relationships, while Transformers are designed to handle such relationships. CNNs use convolutional layers to extract spatial features, whereas Transformers employ self-attention mechanisms to capture dependencies between elements in a sequence. This difference in architecture makes CNNs suitable for vision tasks and Transformers for natural language processing.\n",
      "----------------------\n",
      "Relevant chunks retrieved: [Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='(also called kernels) over the input data to extract spatial features. Each filter captures specific patterns, such as edges, textures, or shapes. As data passes through multiple convolutional layers, CNNs learn hierarchical representations — from low-level details to high-level semantic features.\\n\\nA typical CNN architecture includes several key components: convolutional layers for feature extraction, pooling layers for dimensionality reduction, activation functions (like ReLU) for'), Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='improving performance. ResNet’s introduction of residual connections solved the vanishing gradient problem, enabling extremely deep networks. CNNs are now integrated into edge devices, autonomous systems, and healthcare diagnostics due to their high efficiency and accuracy.\\n\\nWhile CNNs excel at spatial understanding, they have limitations in handling sequential or contextual relationships, which paved the way for architectures like Transformers. Still, CNNs remain foundational in vision AI,'), Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='reduction, activation functions (like ReLU) for non-linearity, and fully connected layers for decision-making. Pooling operations, such as max pooling, reduce computation and help achieve translation invariance. Batch normalization and dropout are often used to improve training stability and reduce overfitting.\\n\\nPopular CNN architectures like LeNet-5, AlexNet, VGGNet, ResNet, and EfficientNet have progressively increased depth and complexity while improving performance. ResNet’s introduction of')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the difference between Convolutional Neural Network and Transformer architecture?\"\n",
    "print(\"Response:\",rag_chain.invoke(query))\n",
    "print(\"----------------------\")\n",
    "print(\"Relevant chunks retrieved:\",retriever.get_relevant_documents(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05405440",
   "metadata": {},
   "source": [
    "### Conversational memory + RAG ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8285c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e12ced84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027F38C14830>, search_kwargs={'k': 3}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027F79E9A7A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question         which might reference context in the chat history, formulate a standalone question         which can be understood without the chat history. Do NOT answer the question,         just reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000027F47A45A70>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027F38C14830>, search_kwargs={'k': 3})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction1 = \"\"\"Given a chat history and the latest user question \\\n",
    "        which might reference context in the chat history, formulate a standalone question \\\n",
    "        which can be understood without the chat history. Do NOT answer the question, \\\n",
    "        just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "instruction_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", instruction1),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")])\n",
    "### create history aware retriever ##\n",
    "history_aware_retr = create_history_aware_retriever(\n",
    "    llm_model, retriever, instruction_prompt\n",
    ")\n",
    "\n",
    "'''\n",
    "Working :\n",
    "-It replaces the input and chat_history placeholders in the prompt with specified values, creating a new ready-to-use prompt that essentially says \"Take this chat history and this last input, and rephrase the last input in a way that anyone can understand it without seeing the chat history\".\n",
    "-It sends the new prompt to the LLM and receives a rephrased input.\n",
    "-It then sends the rephrased input to the vector store retriever and receives a list of documents relevant to this rephrased input.\n",
    "'''\n",
    "history_aware_retr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b2aeadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an intelligent assisstant, for question-answering tasks,\n",
    "Use the retrieved context given below to answer the question in a more streamlined way.\n",
    "Answer in only 3 sentences\n",
    "\n",
    "Context : {context}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "questn_answer_chain = create_stuff_documents_chain(\n",
    "    llm_model,prompt\n",
    ")\n",
    "conversational_ragChain = create_retrieval_chain(\n",
    "\n",
    "    history_aware_retr,questn_answer_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b97b4182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027F79E9A7A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are an intelligent assisstant, for question-answering tasks,\\nUse the retrieved context given below to answer the question in a more streamlined way.\\nAnswer in only 3 sentences\\n\\nContext : {context}\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000027F47A45A70>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questn_answer_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00573358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027F38C14830>, search_kwargs={'k': 3}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027F79E9A7A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question         which might reference context in the chat history, formulate a standalone question         which can be understood without the chat history. Do NOT answer the question,         just reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "           | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000027F47A45A70>, default_metadata=(), model_kwargs={})\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000027F38C14830>, search_kwargs={'k': 3})), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000027F79E9A7A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are an intelligent assisstant, for question-answering tasks,\\nUse the retrieved context given below to answer the question in a more streamlined way.\\nAnswer in only 3 sentences\\n\\nContext : {context}\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000027F47A45A70>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_ragChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bcc1d3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [],\n",
       " 'input': 'What is a CNN architecture?',\n",
       " 'context': [Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='(also called kernels) over the input data to extract spatial features. Each filter captures specific patterns, such as edges, textures, or shapes. As data passes through multiple convolutional layers, CNNs learn hierarchical representations — from low-level details to high-level semantic features.\\n\\nA typical CNN architecture includes several key components: convolutional layers for feature extraction, pooling layers for dimensionality reduction, activation functions (like ReLU) for'),\n",
       "  Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='reduction, activation functions (like ReLU) for non-linearity, and fully connected layers for decision-making. Pooling operations, such as max pooling, reduce computation and help achieve translation invariance. Batch normalization and dropout are often used to improve training stability and reduce overfitting.\\n\\nPopular CNN architectures like LeNet-5, AlexNet, VGGNet, ResNet, and EfficientNet have progressively increased depth and complexity while improving performance. ResNet’s introduction of'),\n",
       "  Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='improving performance. ResNet’s introduction of residual connections solved the vanishing gradient problem, enabling extremely deep networks. CNNs are now integrated into edge devices, autonomous systems, and healthcare diagnostics due to their high efficiency and accuracy.\\n\\nWhile CNNs excel at spatial understanding, they have limitations in handling sequential or contextual relationships, which paved the way for architectures like Transformers. Still, CNNs remain foundational in vision AI,')],\n",
       " 'answer': 'CNN architecture uses convolutional layers to extract features, pooling layers for dimensionality reduction, and activation functions for non-linearity. It also uses fully connected layers for decision-making. Architectures like ResNet have increased depth and complexity while improving performance.'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "response1 = conversational_ragChain.invoke(\n",
    "    {\"chat_history\":chat_history,\n",
    "     \"input\":\"What is a CNN architecture?\"}\n",
    ")\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8fe6aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='What is a CNN architecture?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='CNN architecture uses convolutional layers to extract features, pooling layers for dimensionality reduction, and activation functions for non-linearity. It also uses fully connected layers for decision-making. Architectures like ResNet have increased depth and complexity while improving performance.', additional_kwargs={}, response_metadata={})],\n",
       " 'input': 'Where can it be used?',\n",
       " 'context': [Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='(also called kernels) over the input data to extract spatial features. Each filter captures specific patterns, such as edges, textures, or shapes. As data passes through multiple convolutional layers, CNNs learn hierarchical representations — from low-level details to high-level semantic features.\\n\\nA typical CNN architecture includes several key components: convolutional layers for feature extraction, pooling layers for dimensionality reduction, activation functions (like ReLU) for'),\n",
       "  Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='reduction, activation functions (like ReLU) for non-linearity, and fully connected layers for decision-making. Pooling operations, such as max pooling, reduce computation and help achieve translation invariance. Batch normalization and dropout are often used to improve training stability and reduce overfitting.\\n\\nPopular CNN architectures like LeNet-5, AlexNet, VGGNet, ResNet, and EfficientNet have progressively increased depth and complexity while improving performance. ResNet’s introduction of'),\n",
       "  Document(metadata={'source': 'data//convolutional_neural_networks.txt'}, page_content='improving performance. ResNet’s introduction of residual connections solved the vanishing gradient problem, enabling extremely deep networks. CNNs are now integrated into edge devices, autonomous systems, and healthcare diagnostics due to their high efficiency and accuracy.\\n\\nWhile CNNs excel at spatial understanding, they have limitations in handling sequential or contextual relationships, which paved the way for architectures like Transformers. Still, CNNs remain foundational in vision AI,')],\n",
       " 'answer': 'Answer: CNNs are now integrated into edge devices, autonomous systems, and healthcare diagnostics due to their high efficiency and accuracy.'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.extend([HumanMessage(content=response1['input']),\n",
    "                     AIMessage(content=response1['answer'])])\n",
    "response2 = conversational_ragChain.invoke(\n",
    "    {\"chat_history\":chat_history,\n",
    "     \"input\":\"Where can it be used?\"}\n",
    ")\n",
    "response2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
