{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a32407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader,DirectoryLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb2fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded : 3\n",
      "Metadata of the document : {'source': 'data\\\\chatgpt_conversational_ai.txt'}\n",
      "Document content:\n",
      "ChatGPT and Conversational AI\n",
      "\n",
      "ChatGPT is one of the most widely known conversational AI models deve\n",
      "-------------------------------\n",
      "Metadata of the document : {'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Document content:\n",
      "Google Gemini and the Rise of Multimodal AI\n",
      "\n",
      "Google Gemini represents a new generation of large-scal\n",
      "-------------------------------\n",
      "Metadata of the document : {'source': 'data\\\\machine_learning_fundamentals.txt'}\n",
      "Document content:\n",
      "Machine Learning Fundamentals\n",
      "\n",
      "Machine Learning (ML) is a branch of artificial intelligence focused \n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "## document loaders ##\n",
    "loader = DirectoryLoader(\n",
    "    path='data',\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'}\n",
    ")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents loaded : {len(documents)}\")\n",
    "for index,doc in enumerate(documents):\n",
    "    print(f\"Metadata of the document : {doc.metadata}\")\n",
    "    print(f\"Document content:\\n{doc.page_content[0:100]}\")\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07bd0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created : 12\n",
      "Content in chunk : architecture but extends it with multimodal embeddings. These embeddings allow the model to represen\n",
      "Metadata in a chunk:{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n"
     ]
    }
   ],
   "source": [
    "## document splitters ##\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size = 500, ##max size of one chunk\n",
    "    chunk_overlap = 50, ##chunk overlap character count\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of chunks created : {len(chunks)}\")\n",
    "print(f\"Content in chunk : {chunks[6].page_content[0:100]}\")\n",
    "print(f\"Metadata in a chunk:{chunks[6].metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e904d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 12 vectors\n"
     ]
    }
   ],
   "source": [
    "### chromadb vector store ###\n",
    "persist_dir = \"./chroma_db\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2') ##embedding model\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_dir,\n",
    "    collection_name='rag_collection'\n",
    ")\n",
    "print(f\"Vector store created with {vector_store._collection.count()} vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55fb94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number-1\n",
      "chunk metadata:\n",
      "{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Data in chunk:\n",
      "to systems capable of general intelligence. Gemini’s release positions Google as a key competitor to OpenAI’s GPT-4 and Anthropic’s Claude models. As multimodal AI continues to mature, we can expect breakthroughs in robotics, education, and accessibility—where machines can truly “see,” “hear,” and “understand” the world like humans do.\n",
      "----------------------------\n",
      "chunk number-2\n",
      "chunk metadata:\n",
      "{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Data in chunk:\n",
      "traditional LLMs that are text-only, Gemini can interpret visual inputs, perform cross-modal reasoning, and produce outputs that combine modalities. This makes it ideal for use cases like visual question answering, document analysis, or creative generation. It also integrates with Google Search, Workspace, and YouTube, enabling intelligent summarization, content generation, and recommendation systems.\n",
      "\n",
      "Technically, Gemini builds upon the Transformer architecture but extends it with multimodal\n",
      "----------------------------\n",
      "chunk number-3\n",
      "chunk metadata:\n",
      "{'source': 'data\\\\google_gemini_multimodal_ai.txt'}\n",
      "Data in chunk:\n",
      "Google Gemini and the Rise of Multimodal AI\n",
      "\n",
      "Google Gemini represents a new generation of large-scale multimodal models designed to handle text, images, audio, and video simultaneously. Developed by Google DeepMind, Gemini combines the strengths of language and vision models into a unified architecture. This allows it to not only understand text but also analyze images, generate captions, and even reason about data from multiple sources.\n",
      "\n",
      "Unlike traditional LLMs that are text-only, Gemini can\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "## testing the similarity search##\n",
    "\n",
    "query = 'What capabilities does Google gemini have?'\n",
    "sim_chunks = vector_store.similarity_search(query,k=3)\n",
    "for index,chnk in enumerate(sim_chunks):\n",
    "    print(f\"chunk number-{index+1}\")\n",
    "    print(f\"chunk metadata:\\n{chnk.metadata}\")\n",
    "    print(f\"Data in chunk:\\n{chnk.page_content}\")\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
