{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339754e1",
   "metadata": {},
   "source": [
    "## AGENTIC RAG\n",
    "1. 2 vector databases - FAISS, ChromaDB <> 2 retrievers\n",
    "2. 1 Wikipedia Loader, 1 ArxivLoader\n",
    "3. Rewriter\n",
    "4. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7885827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, DirectoryLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS,Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate,SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap,RunnableLambda,RunnableParallel,RunnablePassthrough\n",
    "from typing import List,TypedDict,Literal\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import END,StateGraph\n",
    "from langchain.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel,Field\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm_model = ChatGoogleGenerativeAI(model = 'gemini-2.0-flash',max_retries=2)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2') ##embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ddcff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs loaded : 5\n",
      "Document preview (200 characters) : Agentic AI: The Evolution of Autonomous Intelligence\n",
      "\n",
      "Agentic AI represents the next major step in artificial intelligence—systems that not only respond intelligently but also act autonomously toward \n",
      "Metadata preview : {'source': 'data\\\\agentic_ai.txt'}\n",
      "\n",
      "Number of chunks created : 34\n",
      "Chunks preview (200 characters) : page_content='Agentic AI: The Evolution of Autonomous Intelligence\n",
      "\n",
      "Agentic AI represents the next major step in artificial intelligence—systems that not only respond intelligently but also act autonomously toward goals. Unlike traditional AI, which passively reacts to inputs, agentic AI operates through active reasoning, planning, and tool usage. These systems can decompose complex problems into smaller tasks, make decisions, and interact with external data sources to achieve objectives efficiently.' metadata={'source': 'data\\\\agentic_ai.txt'}\n",
      "Number of vectors stored : 34\n"
     ]
    }
   ],
   "source": [
    "### Retriever Tools ###\n",
    "docs = DirectoryLoader(\n",
    "    path='data',\n",
    "    glob='*.txt',\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'}\n",
    ").load()\n",
    "print(f\"Number of docs loaded : {len(docs)}\")\n",
    "print(f\"Document preview (200 characters) : {docs[0].page_content[0:200]}\")\n",
    "print(f\"Metadata preview : {docs[0].metadata}\\n\")\n",
    "\n",
    "\n",
    "doc_chunks = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    separators=[\"\\n\",\" \"]\n",
    ").split_documents(docs)\n",
    "\n",
    "print(f\"Number of chunks created : {len(doc_chunks)}\")\n",
    "print(f\"Chunks preview (200 characters) : {doc_chunks[0]}\")\n",
    "\n",
    "## creating chromadb retriever ##\n",
    "persist_dir = './chromadb'\n",
    "vector_store1 = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name='vect_Store2',\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "retriever1 = vector_store1.as_retriever(search_type='mmr',\n",
    "                                      search_kwargs={'k':3})\n",
    "print(f\"Number of vectors stored : {vector_store1._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c533da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs loaded : 5\n",
      "Document preview (200 characters) : Art: The Language of the Soul\n",
      "\n",
      "Art is humanity’s oldest and most universal form of expression. Long before the invention of language, humans painted on cave walls, sang around fires, and danced to rhy\n",
      "Metadata preview : {'source': 'data2\\\\art.txt'}\n",
      "\n",
      "Number of chunks created : 19\n",
      "Chunks preview (200 characters) : page_content='Art: The Language of the Soul\n",
      "\n",
      "Art is humanity’s oldest and most universal form of expression. Long before the invention of language, humans painted on cave walls, sang around fires, and danced to rhythms of the earth. Through art, we translate emotion into form, silence into sound, and imagination into reality.' metadata={'source': 'data2\\\\art.txt'}\n",
      "Number of vectors stored : 19\n"
     ]
    }
   ],
   "source": [
    "### Retriever 2 ###\n",
    "docs2 = DirectoryLoader(\n",
    "    path='data2',\n",
    "    glob='*.txt',\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'}\n",
    ").load()\n",
    "print(f\"Number of docs loaded : {len(docs2)}\")\n",
    "print(f\"Document preview (200 characters) : {docs2[0].page_content[0:200]}\")\n",
    "print(f\"Metadata preview : {docs2[0].metadata}\\n\")\n",
    "\n",
    "\n",
    "doc_chunks2 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    "    separators=[\"\\n\",\" \"]\n",
    ").split_documents(docs2)\n",
    "\n",
    "print(f\"Number of chunks created : {len(doc_chunks2)}\")\n",
    "print(f\"Chunks preview (200 characters) : {doc_chunks2[0]}\")\n",
    "\n",
    "## creating chromadb retriever ##\n",
    "persist_dir = './chromadb'\n",
    "vector_store2 = Chroma.from_documents(\n",
    "    documents=doc_chunks2,\n",
    "    embedding=embedding_model,\n",
    "    collection_name='vect_Store3',\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "retriever2 = vector_store2.as_retriever(search_type='mmr',\n",
    "                                      search_kwargs={'k':3})\n",
    "print(f\"Number of vectors stored : {vector_store2._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created : 97\n"
     ]
    }
   ],
   "source": [
    "### WebLoader <> FAISS vectore store ###\n",
    "url = ['https://www.ey.com/en_in/insights/tax/economy-watch/indian-economy-by-twenty-fifty-in-pursuit-to-achieve-the-thirty-trillion-dollar-mark',\n",
    "       'https://www.pib.gov.in/PressNoteDetails.aspx?NoteId=154840&ModuleId=3',\n",
    "       'https://www.goldmansachs.com/insights/articles/why-the-indian-economy-is-buzzing-with-energy-and-optimism']\n",
    "document3 = [WebBaseLoader(link).load() for link in url]\n",
    "docs3 = [doc for index in document3 for doc in index]\n",
    "\n",
    "\n",
    "doc_chunks3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap = 80,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \"]\n",
    ").split_documents(docs3)\n",
    "\n",
    "print(f\"Number of chunks created : {len(doc_chunks3)}\")\n",
    "## creating chromadb retriever ##\n",
    "vector_store3 = FAISS.from_documents(documents=doc_chunks3,\n",
    "                                     embedding=embedding_model)\n",
    "retriever3 = vector_store3.as_retriever(search_type='mmr',\n",
    "\n",
    "                                      search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab56bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tools ##\n",
    "retriever_tool1 = create_retriever_tool(\n",
    "    retriever=retriever1,\n",
    "    name='retriever_ai_context',\n",
    "    description='Vector store retriever for AI and AI use cases, Information on Machine & Deep Learning & Agentic AI'\n",
    ")\n",
    "\n",
    "retriever_tool2 = create_retriever_tool(\n",
    "    retriever=retriever2,\n",
    "    name='retriever_art_context',\n",
    "    description='Vector store retriever for Art & History, Human Mind etc'\n",
    ")\n",
    "\n",
    "retriever_tool3 = create_retriever_tool(\n",
    "    retriever=retriever3,\n",
    "    name='retriever_indian_economy',\n",
    "    description='Vector store retriever for Indian Economy'\n",
    ")\n",
    "\n",
    "## wikipedia and arxiv tools ##\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper = WikipediaAPIWrapper())\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=ArxivAPIWrapper())\n",
    "\n",
    "## taviliy web search tools ##\n",
    "search_tool = TavilySearch(search_depth=\"advanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e390e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a228731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
